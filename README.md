# HOW BOOTCAMPS - ENGENHARIA DE DADOS

A ideia dessa pasta √© agrupar todos os conhecimentos adquiridos durante as aulas e com os c√≥digos hands-on dos projetos ensinados pelos facilitadores.

## üßëüèª‚Äçüíª DESCRITIVOS DAS PASTAS E PROJETOS

* **A001 (Fundamentos da Engenharia de dados)**: O objetivo do exerc√≠cio contido na pasta √© de compreender a din√¢mica inicial de um Engenheiro de Dados, extraindo dados de um site e realizando os procedimentos necess√°rios para inseri-los num dataframe.
  * *main.py*: A proposta do desafio √© coletar dados dos sorteios da Lotof√°cil e levantar informa√ß√µes relativas √†:
    * Quais n√∫meros mais sorteados e menos sorteados?
    * Quais combina√ß√µes de n√∫meros pares e √≠mpares e primos saem por sorteio?

    üéØ Atingindo os **objetivos**:
    * Criar um ambiente virtual.
    * Ler um arquivo de dados para um dataframe.
    * Tratar a informa√ß√£o recebida.
    * Selecionar os dados necess√°rios.
    * Extrair informa√ß√µes dos dados (em `html`).
    * Analisar os dados.
    * Automatizar o processo.
  
    üìñ **Bibliotecas** utilizadas: `requests`, `pandas` e `collections`.

* **A002 (Fundamentos da Ingest√£o de dados)**: A ideia do m√≥dulo √© entender o funcionamento de APIs e requests, tratando erros e fazendo retentativas, criar logs, utilizar o Chrome Inspect, buscando dados atrav√©s dessa inspe√ß√£o.
  * *main.py*: A ideia do exerc√≠cio/desafio √© entender como funciona uma API e que tipo de direcionamento pode ser feito com os dados extra√≠dos. A API utilizada foi de convers√£o de moedas.
    üéØ Atingindo os **objetivos**:
    * Criar um ambiente virtual.
    * Coletar dados utilizando API.
    * Retornar dados necess√°rios (abstra√ß√£o).
    * Tratar erros com `try` e `except`.
    * Utilizar decoradores e tratar erros por meio do `on_exception` da biblioteca `backoff`.
    * Criar logs para acompanhamento dos dados ingeridos.

    üìñ **Bibliotecas** utilizadas: `requests`, `json`, `backoff`, `random` e `logging`.
  
  * *podcast.py*: Extrair todos os epis√≥dios de podcast do site *portalcafebrasil* e inserir num dataframe com colunas de nome e descri√ß√£o.
    üéØ Atingindo os **objetivos**:
    * Coletar dados do site.
    * Utilizar o `BeaufifulSoup` para extrair informa√ß√µes de html do site.
    * Utilizar o Google Inspect.
    * Criar logs para acompanhamento dos dados ingeridos.
    * Criar dataframe e inserir os dados extra√≠dos do site.
    * Salvar o arquivo em .csv.
   * *imoveis.py*: Extrair os 9.500 apartamentos para venda mais atualizados de Jo√£o Pessoa do site Viva Real e inserir as informa√ß√µes num dataframe.
    üéØ Atingindo os **objetivos**:
    * Coletar dados do site.
    * Utilizar o Google Inspect.
    * Utilizar o `Thunder Client` (extens√£o do VSCode) para organizar informa√ß√µes extra√≠das do retorno da API.
    * Utilizar o `BeaufifulSoup` dentro da minha fun√ß√µa para retirar informa√ß√µes do html extra√≠do pelo `Thunder Client`.
    * Evitar timeout de requisi√ß√µes.
    * Criar dataframe e inserir os dados extra√≠dos do site.
    * Salvar o arquivo em .csv.

    üìñ **Bibliotecas** utilizadas: `os`, `wsgiref.validate`, `pyparsing`, `requests`, `bs4`, `pandas`, `wcwidth`, `json` e `time`.

*üöß Em atualiza√ß√£o üöß*
