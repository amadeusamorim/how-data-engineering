# HOW BOOTCAMPS - ENGENHARIA DE DADOS

A ideia dessa pasta Ã© agrupar todos os conhecimentos adquiridos durante as aulas e com os cÃ³digos hands-on dos projetos ensinados pelos facilitadores.

## ğŸ§‘ğŸ»â€ğŸ’» DESCRITIVOS DAS PASTAS E PROJETOS

* **A001 (Fundamentos da Engenharia de dados)**: O objetivo do exercÃ­cio contido na pasta Ã© de compreender a dinÃ¢mica inicial de um Engenheiro de Dados, extraindo dados de um site e realizando os procedimentos necessÃ¡rios para inseri-los num dataframe.
  * *main.py*: A proposta do desafio Ã© coletar dados dos sorteios da LotofÃ¡cil e levantar informaÃ§Ãµes relativas Ã :
    * Quais nÃºmeros mais sorteados e menos sorteados?
    * Quais combinaÃ§Ãµes de nÃºmeros pares e Ã­mpares e primos saem por sorteio?

    ğŸ¯ Atingindo os **objetivos**:
    * Criar um ambiente virtual.
    * Ler um arquivo de dados para um dataframe.
    * Tratar a informaÃ§Ã£o recebida.
    * Selecionar os dados necessÃ¡rios.
    * Extrair informaÃ§Ãµes dos dados (em `html`).
    * Analisar os dados.
    * Automatizar o processo.
  
    ğŸ“– **Bibliotecas** utilizadas: `requests`, `pandas` e `collections`.

* **A002 (Fundamentos da IngestÃ£o de dados)**: A ideia do mÃ³dulo Ã© entender o funcionamento de APIs e requests, tratando erros e fazendo retentativas, criar logs, utilizar o Chrome Inspect, buscando dados atravÃ©s dessa inspeÃ§Ã£o.
  * *main.py*: A ideia do exercÃ­cio/desafio Ã© entender como funciona uma API e que tipo de direcionamento pode ser feito com os dados extraÃ­dos. A API utilizada foi de conversÃ£o de moedas.
    ğŸ¯ Atingindo os **objetivos**:
    * Criar um ambiente virtual.
    * Coletar dados utilizando API.
    * Retornar dados necessÃ¡rios (abstraÃ§Ã£o).
    * Tratar erros com `try` e `except`.
    * Utilizar decoradores e tratar erros por meio do `on_exception` da biblioteca `backoff`.
    * Criar logs para acompanhamento dos dados ingeridos.

    ğŸ“– **Bibliotecas** utilizadas: `requests`, `json`, `backoff`, `random` e `logging`.
  
  * *podcast.py*: Extrair todos os episÃ³dios de podcast do site *portalcafebrasil* e inserir num dataframe com colunas de nome e descriÃ§Ã£o.
    ğŸ¯ Atingindo os **objetivos**:
    * Coletar dados do site.
    * Utilizar o `BeautifulSoup` para extrair informaÃ§Ãµes de html do site.
    * Utilizar o Google Inspect.
    * Criar logs para acompanhamento dos dados ingeridos.
    * Criar dataframe e inserir os dados extraÃ­dos do site.
    * Salvar o arquivo em .csv.

    ğŸ“– **Bibliotecas** utilizadas: `requests`, `bs4`, `logging` e `pandas`.

   * *imoveis.py*: Extrair os 9.500 apartamentos para venda mais atualizados de JoÃ£o Pessoa do site Viva Real e inserir as informaÃ§Ãµes num dataframe.
    ğŸ¯ Atingindo os **objetivos**:
      * Coletar dados do site.
      * Utilizar o Google Inspect.
      * Utilizar o `Thunder Client` (extensÃ£o do VSCode) para organizar informaÃ§Ãµes extraÃ­das do retorno da API.
      * Utilizar o `BeautifulSoup` dentro da minha funÃ§Ãµa para retirar informaÃ§Ãµes do html extraÃ­do pelo `Thunder Client`.
      * Evitar timeout de requisiÃ§Ãµes.
      * Criar dataframe e inserir os dados extraÃ­dos do site.
      * Salvar o arquivo em .csv.

    ğŸ“– **Bibliotecas** utilizadas: `os`, `wsgiref.validate`, `pyparsing`, `requests`, `bs4`, `pandas`, `wcwidth`, `json` e `time`.

*ğŸš§ Em atualizaÃ§Ã£o ğŸš§*
